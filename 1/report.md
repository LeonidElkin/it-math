# Task 1. OpenMP

## Предположение

Введем следующие обозначения:

- $`N`$ - Размер стороны сетки не включая границы
- $`BS`$ - Размер блока в параллельном блочном алгоритме(11.6)
- $`t(x)`$ - Время выполнения в условных единицах
- $`t_n(x)`$ - Время выполнения в условных единицах на $`n`$ потоках
- $`\epsilon`$ - Точность вычисления сетки, отображает как сильно значения в ней могут отличаться от истины
- $`iter_{\epsilon}`$ - Количество итераций внешнего цикла, напрямую зависит от эпсилон. Не может быть указано явно.
На параллельных версиях количество итераций может варьироваться от запуска к запуску 
- $`ceil(x)`$ - Округление $`x`$ в большую сторону
- $`BN = ceil((N+2) / BS)`$ - Размер стороны сетки в блоках. Если размер блока не делиться нацело на размер сетки,
остаток также будет считаться блоком, отсюда и окргуление в большую сторону

> Далее время будет высчитываться с учетом констант, которые в наших экспериментах значительно в процентном соотношении влияют на производительность

### Время для алгоритма 11.1 (последовательный)  

$`t(grid\_calculation) \approx iter * N^2 * C_1, \ C_1 \in R; \ O(iter*N^2)`$ - Где $`C_1`$ - это дополнительные вычисления на каждой итерации

### Время для алгоритма 11.3 (параллельный без блоков, без гарантии однозначности)

$`t_1(grid\_calculation) = iter*(N^2 * C_2 + N * C), \ C_2 \in R`$\ - Где $`C_2`$ - это дополнительные вычисления на каждой итерации, 
а также затраты на организацию многопоточности, которые выполняются независимо от того, что выставлен один поток. $`N * c`$ пересчет дельты

$`t_n(grid\_calculation) \approx iter*(ceil(N/n)* N * C_3 + ceil(N/n) * C), \ C_3 \in R`$ - Где $`C_3`$ - это дополнительные вычисления на каждой итерации, 
а также затраты на организацию многопоточности и синхронизация самих потоков. $`ceil(N/n) * c`$ пересчет дельты

### Замечание

Заметим, что $`C_1 \le C_2 \le C_3`$ так как включает в себя гораздо больше $`O(1)`$ операций

> Далее нумерация констант не связан с предыдущими. Также было дано исчерпывающее обоснование констант, далее всё аналогично

### Время для алгоритма 11.3 (параллельный с блоками, с большей однозначностью)

$`t(block\_calculation) \approx BS^2 * C, \ C \in R`$ - Вычисление блока выполняется последовательно 

$`t_1(grid\_calculation) \approx iter*(t(block\_calculation)*BN^2 * C_1 + BN * C_2), \ C_1, C_2 \in R`$

$`t_n(grid\_calculation) \approx iter*({\displaystyle\sum_{i = 1}^{BN}t(block\_calculation) * ceil(i/n) * С_1} +
{\displaystyle\sum_{i = 1}^{BN - 1} t(block\_calculation) * ceil(i/n) * C_2} + C_3 * ceil(BN/n)), \ C_1, C_2, C_3 \in R`$ - 
Где $`ceil(i/n)`$ - это время на параллельную обработку блоков. $`C_3 * BN`$ - подсчёт максимальной дельты 

> После некоторых упрощений

$`t_n(grid\_calculation) \approx iter * (2 * t(block\_calculation) * C_1 * \displaystyle\sum_{i = 1}^{BN - 1} ceil(i/n) +
ceil(BN/n) * C_2 *(t(block\_calculation) + 1)), \ C_1, C_2 \in R`$ - Где $`C_1`$ - это константа отображающая и подъём, и спад волны, а $`C_2`$ - 
это $`C_3`$ из прошлой формулы

### Вывод

На основе выведенных форму была построена [табличка](https://docs.google.com/spreadsheets/d/19KujCMgUAl84SRhnpacT32TeKRqqjj8jBjQOnZdL_qg/edit?usp=sharing)

## Тесты производительности 

Тесты производились на `WSL Ubuntu 22.04` в окружении `Windows 11 23H2` на процессоре `11th Gen Intel(R) Core(TM) i7-11800H` (8 ядер)

При компиляции оптимизации не использовались

Каждый алгоритм запускался множество раз с разными параметрами. 
Также каждые алгоритм с одинаковыми параметрами воспроизводился 20 раз и высчитывалось среднее время и количество итераций для уменьшения погрешности. 
Результаты тестов можно посмотреть в `csv` формате в папке `py_results`, а также в таблицах: 
[blocky](https://docs.google.com/spreadsheets/d/1nD-hFYgWMGN4Kbm7gvQ9CQXyz2Dfr6qu7SqAIyZRM3Y/edit?usp=sharing), 
[sequential](https://docs.google.com/spreadsheets/d/1Scenntbu1RFW9AaJeEJoNyQUI75qAOfT4XVxaaSClYI/edit?usp=sharing), 
[parallel](https://docs.google.com/spreadsheets/d/1qQzMN_60zjYhmKp7Rhx6T5C7xJBlTdyTCX7vLpu3tH8/edit?usp=sharing) 

Тесты проводились на функциях представленных в книге. Можно провести тестирование и на других функциях, код позволяет их добавить и легко протестировать,
но для поддержания той же точности, мне бы потребовалось гонять свою машину неделю :)

Сравнение теории и практики прикладываю также в [табличке](https://docs.google.com/spreadsheets/d/1BvvNS07ShoGIE78ogfZFM7GtiU24qdLCQ4y0k3ieWno/edit?usp=sharing)

# Результаты и анализ

Как можно заметить, результаты +- соотносятся с теорией, а порой даже превосходят, что, наверное, говорит, не о лучшей точности расчётов, однако, они примерно
отражают реальность.

Также хочется отметить, что прирост между 4 и 8 потоками крайне незначительный. Могу списать это на то, что ядра с 5 по 8 не полностью используются в расчётах, 
а лишь часть их времени уделяются программе. Остальное уходят на обработку системных сигналов и тд. Также затраты на синхронизацию 8 потоков намного больше, 
чем 4, не уверен насколько это критично, но факт есть факт. Все пользовательские программы во время тестирования были выключены.

Прошу также отметить, что при увеличении размера сетки, увеличивается и прирост. Это можно оправдать тем, что синхронизация многопоточности на маленьких сетках
занимает большую часть в процентах от последовательного выполнения алгоритма, и хотя мы видим прирост даже при 100, он крайне незначительный, и то, на многих потоках.

В среднем блочный алгоритм всегда работал лучше, чем параллельный без блоков, что вообще не соотносится с расчётами. Параллельный давал ускорение в среднем в 2 раза хуже, 
чем предпологалось, однако в абсолютных значениях, выглядит довольно неплохо. Могу списать такие результаты либо на плохую синхронизацию потоков при работе с точками, 
а не блоками, либо на отсутствие оптимизации при компиляции, либо на мои кривые руки при кодинге алгоритма

И еще, параллельный алгоритм без блоков при увеличении размера сетки терял свою эффективость, в то время как блочный наоборот набирал обороты
